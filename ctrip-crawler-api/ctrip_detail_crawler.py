import asyncio
import json
import re
from datetime import datetime
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

def extract_product_data_from_markdown(markdown_content):
    """
    ä»markdownå†…å®¹ä¸­æå–ç»“æ„åŒ–çš„å•†å“æ•°æ®
    """
    print("ğŸ¯ å¼€å§‹ä»markdownä¸­æå–ç»“æ„åŒ–æ•°æ®...")
    
    # åˆå§‹åŒ–æ•°æ®ç»“æ„
    product_data = {
        "product_id": "",
        "title": "",
        "subtitle": "",
        "route_title": "",
        "route_overview": {
            "transport": "",
            "accommodation": "",
            "spots": "",
            "meals": "",
            "activities": "",
        },
        "rating": 0,
        "sales_count": 0,
        "review_count": 0,
        "price": "",
        "product_images": [],  # å•†å“å±•ç¤ºå›¾ï¼ˆå»æ‰å°ºå¯¸å‚æ•°çš„å¤§å›¾ï¼‰
        "detail_images": [],   # è¯¦æƒ…ä»‹ç»å›¾ï¼ˆä¿æŒåŸå°ºå¯¸ï¼‰
        "overview": {
            "guide": "",
            "transport": "",
            "activities": "",
            "accommodation": "",
            "meals": ""
        },
        "features": [],
        "cost_info": {
            "transport": "",
            "accommodation": "",
            "meals": "",
            "tickets": "",
            "service": ""
        }
    }
    
    lines = markdown_content.split('\n')

    # for line in lines:
    #     print(line)

    # 1. æå–é”€å”®é‡ã€è¯„åˆ†ã€è¯„ä»·æ•°
    sales_count_pattern = r'å·²å”®(\d+)äºº'
    rating_pattern = r'(\d(\.\d)?)åˆ†.+'
    review_count_pattern = r'(\d+)æ¡!\[\]\(.+'
    for line in lines:
        line = line.strip()
        if not line:
            continue

        sales_count_match = re.search(sales_count_pattern, line)
        if sales_count_match:
            sales_count = int(sales_count_match.group(1))
            product_data["sales_count"] = sales_count
            continue

        rating_match = re.search(rating_pattern, line)
        if rating_match:
            rating = float(rating_match.group(1))
            product_data["rating"] = rating
            continue

        review_count_match = re.search(review_count_pattern, line)
        if review_count_match:
            review_count = int(review_count_match.group(1))
            product_data["review_count"] = review_count
            break


    # 2. æå–çº¿è·¯å­—æ¯ï¼Œå¹¶æå–çº¿è·¯æ ‡é¢˜å’Œæ¦‚è¿°
    route_letter_pattern = r'([A-Z])çº¿'
    for i, line in enumerate(lines):
        line = line.strip()
        if not line:
            continue

        match = re.search(route_letter_pattern, line)
        if match:
            route_letter = match.group(1)
            for j in range(i + 1, len(lines)):
                next_line = lines[j].strip()
                if not next_line:
                    continue

                if next_line.startswith(f'{route_letter}çº¿|'):
                    product_data["route_title"] = next_line
                    for k in range(j + 1, len(lines)):
                        next_line = lines[k].strip()
                        if not next_line:
                            continue

                        print(next_line)
                        print(lines[k+1].strip())
                        if re.match(r'[A-Z]çº¿\|', next_line):
                            break
                        elif next_line == "è¡Œ":
                            product_data["route_overview"]["transport"] = lines[k + 1]
                        elif next_line == "ä½":
                            product_data["route_overview"]["accommodation"] = lines[k + 1]
                        elif next_line == "æ¸¸":
                            product_data["route_overview"]["spots"] = lines[k + 1]
                        elif next_line == "é¤":
                            product_data["route_overview"]["meals"] = lines[k + 1]
                        elif next_line == "æ´»":
                            product_data["route_overview"]["activities"] = lines[k + 1]


    title_pattern = r'.+\+.+ç§å®¶å›¢'
    subtitle_pattern = r'.+Â·.+'
    
    # 1. æå–å•†å“IDå’Œæ ‡é¢˜
    for i, line in enumerate(lines):
        # æå–å•†å“ID (ä»URLä¸­)
        if 'productid=' in line:
            match = re.search(r'productid=(\d+)', line)
            if match:
                product_data["product_id"] = match.group(1)
        
        # æå–æ ‡é¢˜ (é€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªè¾ƒé•¿çš„æ–‡æœ¬è¡Œ)
        if not product_data["title"] and '+' in line:
            match = re.search(title_pattern, line.strip())
            if match:
                product_data["title"] = line.strip()
                # å¯»æ‰¾çœŸæ­£çš„å‰¯æ ‡é¢˜ï¼ˆæ™¯ç‚¹åˆ—è¡¨ï¼‰
                for j in range(i + 1, min(i + 10, len(lines))):
                    next_line = lines[j].strip()
                    # å‰¯æ ‡é¢˜é€šå¸¸åŒ…å«æ™¯ç‚¹åç§°ï¼Œç”¨Â·åˆ†éš”
                    if next_line and 'Â·' in next_line:
                        match = re.search(subtitle_pattern, next_line)
                        if match:
                        # # æ£€æŸ¥æ˜¯å¦åŒ…å«æ™¯ç‚¹å…³é”®è¯
                        # if any(keyword in next_line for keyword in ['ç¾Šå“é›é”™', 'ç å³°', 'é™ˆå¡˜æ²Ÿ', 'å¡è‹¥æ‹‰', 'ç™½å±…å¯º']):
                            product_data["subtitle"] = next_line
                            break
    
    # 2. æå–ä»·æ ¼ä¿¡æ¯
    price_pattern = r'Â¥(\d+)'
    for line in lines:
        if 'Â¥' in line and 'æ˜ŸæœŸ' in line:
            matches = re.findall(price_pattern, line)
            if matches:
                product_data["price"] = f"Â¥{matches[0]}"
                break
    
    # 3. æå–å¹¶åˆ†ç±»å›¾ç‰‡
    image_pattern = r'!\[\]\((https://dimg04\.c-ctrip\.com/images/[^)]+)\)'
    
    def remove_size_params(url):
        """å»æ‰å›¾ç‰‡URLä¸­çš„å°ºå¯¸å‚æ•°ï¼Œè·å–å¤§å›¾"""
        # å»æ‰ _C_æ•°å­—_æ•°å­—_R1_Q80 è¿™æ ·çš„å°ºå¯¸å‚æ•°
        url = re.sub(r'_[A-Za-z]_\d+_\d+(_R\d+_Q\d+)?', '', url)
        return url
    
    def is_detail_image(url):
        """åˆ¤æ–­æ˜¯å¦ä¸ºå•†å“å±•ç¤ºå›¾"""
        # å•†å“å›¾ç‰¹å¾ï¼šé€šå¸¸åŒ…å«ç‰¹å®šçš„å°ºå¯¸å‚æ•°ï¼Œä¸”åœ¨é¡µé¢å‰éƒ¨å‡ºç°
        # è¯¦æƒ…å›¾ç‰¹å¾ï¼šé€šå¸¸æ˜¯.jpgæ ¼å¼ï¼Œæˆ–è€…æ²¡æœ‰å¤æ‚çš„å°ºå¯¸å‚æ•°
        size_pattern = r'_[A-Za-z]_\d+_\d+'
        size_match = re.search(size_pattern, url)
        if size_match:
            return False
        if url.endswith('.jpg'):
            return True  # .jpgé€šå¸¸æ˜¯è¯¦æƒ…å›¾
        return False
    
    processed_images = set()  # ç”¨äºå»é‡
    
    for line in lines:
        matches = re.findall(image_pattern, line)
        for original_url in matches:
            if original_url in processed_images:
                continue
            processed_images.add(original_url)
            
            if is_detail_image(original_url):
                # è¯¦æƒ…å›¾ï¼šä¿æŒåŸå°ºå¯¸
                if original_url not in product_data["detail_images"]:
                    product_data["detail_images"].append(original_url)
            else:
                # å•†å“å›¾ï¼šå»æ‰å°ºå¯¸å‚æ•°ä¿å­˜å¤§å›¾
                large_url = remove_size_params(original_url)
                if large_url not in product_data["product_images"]:
                    product_data["product_images"].append(large_url)
    
    # 4. æå–çº¿è·¯æ€»è§ˆä¿¡æ¯ - å®Œæ•´ä¼˜åŒ–ç‰ˆæœ¬
    overview_section = False
    guide_info_parts = []  # ç”¨äºæ”¶é›†guideçš„å®Œæ•´ä¿¡æ¯
    transport_info_parts = []  # ç”¨äºæ”¶é›†äº¤é€šçš„å®Œæ•´ä¿¡æ¯
    activities_info_parts = []  # ç”¨äºæ”¶é›†æ¸¸ç©çš„å®Œæ•´ä¿¡æ¯
    accommodation_info_parts = []  # ç”¨äºæ”¶é›†ä½å®¿çš„å®Œæ•´ä¿¡æ¯
    meals_info_parts = []  # ç”¨äºæ”¶é›†é¤é£Ÿçš„å®Œæ•´ä¿¡æ¯
    
    for i, line in enumerate(lines):
        if 'çº¿æ€»è§ˆ' in line:
            overview_section = True
            continue
        
        if overview_section and 'äº§å“ç‰¹è‰²' in line:
            overview_section = False
            break
            
        if overview_section:
            if 'å›¢é˜ŸæœåŠ¡' in line:
                # æ”¶é›†å›¢é˜ŸæœåŠ¡çš„å®Œæ•´ä¿¡æ¯
                guide_info_parts = []
                
                # æŸ¥æ‰¾åç»­å‡ è¡Œçš„è¯¦ç»†æè¿°
                for j in range(i+1, min(i+15, len(lines))):
                    desc_line = lines[j].strip()
                    score_pattern = r'\d\.\dåˆ†'

                    # è·³è¿‡ç©ºè¡Œå’Œå›¾ç‰‡é“¾æ¥
                    if not desc_line or desc_line.startswith('![') or desc_line == 'â€¢':
                        continue
                    
                    # æ”¶é›†åŸºæœ¬æœåŠ¡æè¿°
                    if 'å«å¸æœºæœåŠ¡' in desc_line:
                        guide_info_parts.append(desc_line)
                    
                    # æ”¶é›†è¯¦ç»†è¯´æ˜
                    elif 'ä»…å®‰æ’ä¸­æ–‡å¸æœº' in desc_line:
                        guide_info_parts.append(desc_line)
                    
                    # æ”¶é›†å¸æœºä¿¡æ¯
                    elif re.search(score_pattern, desc_line):
                        guide_info_parts.append(lines[j - 1].strip()[:-1])
                    
                    # æ”¶é›†å¸æœºæ•°é‡ä¿¡æ¯
                    elif 'å…¨éƒ¨' in desc_line and 'ä½' in desc_line:
                        guide_info_parts.append(desc_line.strip()[:-1])
                    
                    # æ”¶é›†è¡¥å……è¯´æ˜
                    elif 'ä»¥ä¸Šä¸ºè¿‘æœŸå¸¦è¿‡æœ¬å›¢çš„å¸æœº' in desc_line:
                        guide_info_parts.append(desc_line)
                        break  # æ‰¾åˆ°æœ€åä¸€æ¡ä¿¡æ¯ï¼Œç»“æŸæ”¶é›†
                
                # å°†æ”¶é›†åˆ°çš„ä¿¡æ¯ç»„åˆæˆå®Œæ•´çš„guideæè¿°
                if guide_info_parts:
                    product_data["overview"]["guide"] = '; '.join(guide_info_parts)
                    
            elif 'äº¤é€š' in line:
                # æ”¶é›†äº¤é€šçš„å®Œæ•´ä¿¡æ¯
                transport_info_parts = []
                
                for j in range(i+1, min(i+8, len(lines))):
                    desc_line = lines[j].strip()
                    
                    # è·³è¿‡ç©ºè¡Œå’Œå›¾ç‰‡é“¾æ¥
                    if not desc_line or desc_line.startswith('![') or desc_line == 'â€¢':
                        continue

                    # å¦‚æœé‡åˆ°ä¸‹ä¸€ä¸ªsectionï¼Œåœæ­¢æ”¶é›†
                    if any(keyword in desc_line for keyword in ['æ¸¸ç©', 'ä½å®¿', 'é¤é£Ÿ']):
                        break

                    # æ”¶é›†äº¤é€šç›¸å…³ä¿¡æ¯
                    # if ('å«è¡Œä¸­ä¸“å±ç”¨è½¦' in desc_line or 'ä¸æ‹¼è½¦' in desc_line or
                    #     'ç»æµ' in desc_line and 'åº§' in desc_line):
                    transport_info_parts.append(desc_line.replace('â€¢ ', '').replace(' è¯¦æƒ…', ''))

                if transport_info_parts:
                    product_data["overview"]["transport"] = '; '.join(transport_info_parts)
                    
            elif 'æ¸¸ç©' in line:
                # æ”¶é›†æ¸¸ç©çš„å®Œæ•´ä¿¡æ¯
                activities_info_parts = []
                
                for j in range(i+1, min(i+8, len(lines))):
                    desc_line = lines[j].strip()
                    
                    # è·³è¿‡ç©ºè¡Œå’Œå›¾ç‰‡é“¾æ¥
                    if not desc_line or desc_line.startswith('![') or desc_line == 'â€¢':
                        continue

                    # å¦‚æœé‡åˆ°ä¸‹ä¸€ä¸ªsectionï¼Œåœæ­¢æ”¶é›†
                    if any(keyword in desc_line for keyword in ['ä½å®¿', 'é¤é£Ÿ', 'å›¢é˜ŸæœåŠ¡']):
                        break

                    # æ”¶é›†æ¸¸ç©ç›¸å…³ä¿¡æ¯
                    # if ('ä¸ªæ™¯ç‚¹' in desc_line or 'åœºé¦†' in desc_line or
                    #     'å„¿ç«¥ç¥¨' in desc_line or 'æ— è´­ç‰©' in desc_line):
                    activities_info_parts.append(desc_line.replace('â€¢ ', '').replace(' è¯¦æƒ…', ''))

                if activities_info_parts:
                    product_data["overview"]["activities"] = '; '.join(activities_info_parts)
                    
            elif 'ä½å®¿' in line:
                # æ”¶é›†ä½å®¿çš„å®Œæ•´ä¿¡æ¯
                accommodation_info_parts = []
                
                for j in range(i+1, min(i+8, len(lines))):
                    desc_line = lines[j].strip()
                    
                    # è·³è¿‡ç©ºè¡Œå’Œå›¾ç‰‡é“¾æ¥
                    if not desc_line or desc_line.startswith('![') or desc_line == 'â€¢':
                        continue

                    # å¦‚æœé‡åˆ°ä¸‹ä¸€ä¸ªsectionï¼Œåœæ­¢æ”¶é›†
                    if any(keyword in desc_line for keyword in ['é¤é£Ÿ', 'å›¢é˜ŸæœåŠ¡', 'äº¤é€š']):
                        break

                    # æ”¶é›†ä½å®¿ç›¸å…³ä¿¡æ¯
                    # if ('é’»é…’åº—' in desc_line or 'å¹³æªåº·æ¡‘' in desc_line or
                    #     'è¯¦æƒ…' in desc_line):
                    accommodation_info_parts.append(desc_line.replace('â€¢ ', '').replace(' è¯¦æƒ…', ''))
                
                if accommodation_info_parts:
                    product_data["overview"]["accommodation"] = '; '.join(accommodation_info_parts)
                    
            elif 'é¤é£Ÿ' in line:
                # æ”¶é›†é¤é£Ÿçš„å®Œæ•´ä¿¡æ¯
                meals_info_parts = []
                
                for j in range(i+1, min(i+8, len(lines))):
                    desc_line = lines[j].strip()
                    
                    # è·³è¿‡ç©ºè¡Œå’Œå›¾ç‰‡é“¾æ¥
                    if not desc_line or desc_line.startswith('![') or desc_line == 'â€¢':
                        continue

                    # å¦‚æœé‡åˆ°ä¸‹ä¸€ä¸ªsectionï¼Œåœæ­¢æ”¶é›†
                    if any(keyword in desc_line for keyword in ['äº§å“ç‰¹è‰²', 'å›¢é˜ŸæœåŠ¡', 'äº¤é€š']):
                        break

                    # æ”¶é›†é¤é£Ÿç›¸å…³ä¿¡æ¯
                    # if ('æˆäºº' in desc_line and ('æ—©é¤' in desc_line or 'æ™šé¤' in desc_line or 'è‡ªç†' in desc_line)) or \
                    #    ('å„¿ç«¥' in desc_line and ('æ™šé¤' in desc_line or 'è‡ªç†' in desc_line)):
                    meals_info_parts.append(desc_line.replace('â€¢ ', '').replace(' è¯¦æƒ…', ''))

                if meals_info_parts:
                    product_data["overview"]["meals"] = '; '.join(meals_info_parts)
    
    # 5. æå–äº§å“ç‰¹è‰²
    features_section = False
    for i, line in enumerate(lines):
        if 'äº§å“ç‰¹è‰²' in line:
            features_section = True
            continue
        
        if features_section and ('æ—¥è¡Œç¨‹' in line or 'D1|' in line or 'å±•å¼€å…¨éƒ¨' in line):
            features_section = False
            break
            
        if features_section and line.strip() and not line.startswith('![') and '![' not in line:
            # ç‰¹è‰²é€šå¸¸ä»¥å…³é”®è¯å¼€å¤´ï¼Œå¦‚"å¤§ç‰Œé©¾åˆ°"ã€"ç²¾é€‰é…’åº—"ç­‰
            # if any(keyword in line for keyword in ['å¤§ç‰Œé©¾åˆ°', 'ç²¾é€‰é…’åº—', 'æœåŠ¡ä¿éšœ', 'ç‹¬ç‰¹', 'ç²¾é€‰', 'ä¿éšœ', 'é¦–é€‰', 'åº¦å‡', 'æ™¯ç‚¹']):
            product_data["features"].append(line.strip())
    
    # 6. æå–è´¹ç”¨ä¿¡æ¯ - ä¼˜åŒ–ç‰ˆæœ¬
    cost_section = False
    
    for i, line in enumerate(lines):
        if 'è´¹ç”¨ä¿¡æ¯' in line:
            cost_section = True
            continue
        
        if cost_section and 'è´­ä¹°é¡»çŸ¥' in line:
            cost_section = False
            break
            
        if cost_section and 'è´¹ç”¨åŒ…å«' in line:
            # å¼€å§‹è§£æè´¹ç”¨åŒ…å«é¡¹ç›®
            for j in range(i+1, min(i+20, len(lines))):
                cost_line = lines[j].strip()
                if not cost_line or cost_line.startswith('è´­ä¹°é¡»çŸ¥'):
                    break
                
                if cost_line == 'äº¤é€š':
                    # è·å–ä¸‹ä¸€è¡Œçš„è¯¦ç»†ä¿¡æ¯
                    if j + 1 < len(lines):
                        detail = lines[j + 1].strip()
                        product_data["cost_info"]["transport"] = detail
                elif cost_line == 'ä½å®¿':
                    if j + 1 < len(lines):
                        detail = lines[j + 1].strip()
                        product_data["cost_info"]["accommodation"] = detail
                elif cost_line == 'é¤é£Ÿ':
                    if j + 1 < len(lines):
                        detail = lines[j + 1].strip()
                        product_data["cost_info"]["meals"] = detail
                elif cost_line == 'é—¨ç¥¨åŠåœ°é¢é¡¹ç›®':
                    if j + 1 < len(lines):
                        detail = lines[j + 1].strip()
                        product_data["cost_info"]["tickets"] = detail
                elif cost_line == 'éšå›¢æœåŠ¡äººå‘˜':
                    if j + 1 < len(lines):
                        detail = lines[j + 1].strip()
                        product_data["cost_info"]["service"] = detail
            break
    
    return product_data

async def crawl_and_extract_ctrip_data(url):
    """
    å®Œæ•´çš„æºç¨‹æ‰‹æœºç‰ˆæ•°æ®çˆ¬å–å’Œæå–æµç¨‹
    """
    print("ğŸš€ å¯åŠ¨æºç¨‹æ‰‹æœºç‰ˆå®Œæ•´æ•°æ®çˆ¬å–å™¨...")
    
    # æ‰‹æœºæµè§ˆå™¨é…ç½®
    browser_config = BrowserConfig(
        # browser_type="chromium",
        headless=True,
        # verbose=True,
        # viewport_width=375,
        # viewport_height=812,
        # user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1"
        extra_args=['--disable-web-security']
    )
    
    # çˆ¬å–é…ç½® - é’ˆå¯¹æ‰‹æœºç‰ˆä¼˜åŒ–
    crawler_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        delay_before_return_html=8,  # å‡å°‘å›ºå®šç­‰å¾…æ—¶é—´ï¼š10ç§’ -> 3ç§’
        page_timeout=30000,  # å‡å°‘è¶…æ—¶æ—¶é—´ï¼š60ç§’ -> 30ç§’
        js_code=
            # æ‰‹æœºç‰ˆæ»‘åŠ¨è„šæœ¬ - ä¼˜åŒ–ç‰ˆæœ¬
            """
            // æ‰‹æœºç‰ˆé¡µé¢æ»‘åŠ¨åŠ è½½ - æ€§èƒ½ä¼˜åŒ–ç‰ˆ
            async function mobileScrollOptimized() {
                let totalHeight = 0;
                let distance = 400;  // å¢åŠ æ»‘åŠ¨è·ç¦»ï¼š300 -> 400
                let scrollDelay = 500;  // å‡å°‘æ»‘åŠ¨å»¶è¿Ÿï¼š800ms -> 500ms
                let maxScrolls = 15;  // é™åˆ¶æœ€å¤§æ»‘åŠ¨æ¬¡æ•°ï¼Œé¿å…æ— é™æ»šåŠ¨
                let scrollCount = 0;

                console.log('å¼€å§‹ä¼˜åŒ–æ»‘åŠ¨åŠ è½½...');

                while (totalHeight < document.body.scrollHeight && scrollCount < maxScrolls) {
                    let beforeHeight = document.body.scrollHeight;
                    window.scrollBy(0, distance);
                    totalHeight += distance;
                    scrollCount++;

                    // åŠ¨æ€è°ƒæ•´ç­‰å¾…æ—¶é—´
                    await new Promise(resolve => setTimeout(resolve, scrollDelay));

                    // æ£€æŸ¥æ˜¯å¦æœ‰æ–°å†…å®¹åŠ è½½
                    let afterHeight = document.body.scrollHeight;
                    if (afterHeight === beforeHeight) {
                        // å¦‚æœé¡µé¢é«˜åº¦æ²¡æœ‰å˜åŒ–ï¼Œè¯´æ˜æ²¡æœ‰æ–°å†…å®¹ï¼Œå¯ä»¥æå‰ç»“æŸ
                        console.log('é¡µé¢å†…å®¹å·²å…¨éƒ¨åŠ è½½ï¼Œæå‰ç»“æŸæ»šåŠ¨');
                        break;
                    }

                    // å¦‚æœè¿ç»­3æ¬¡æ»šåŠ¨éƒ½æ²¡æœ‰æ–°å†…å®¹ï¼Œåˆ™ç»“æŸ
                    if (totalHeight >= afterHeight) {
                        console.log('å·²æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨');
                        break;
                    }
                }

                console.log(`æ»šåŠ¨å®Œæˆï¼Œå…±æ»šåŠ¨${scrollCount}æ¬¡`);

                // æ»šåŠ¨åˆ°é¡¶éƒ¨ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
                window.scrollTo(0, 0);
                await new Promise(resolve => setTimeout(resolve, 500));  // å‡å°‘ç­‰å¾…ï¼š1000ms -> 500ms
            }

            await mobileScrollOptimized();
            """
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        print(f"ğŸ“± å¼€å§‹çˆ¬å–æ‰‹æœºç‰ˆé¡µé¢: {url}")
        
        result = await crawler.arun(
            url=url,
            config=crawler_config,
        )
        
        if result.success:
            print("âœ… é¡µé¢çˆ¬å–æˆåŠŸ!")
            
            # ç”Ÿæˆæ—¶é—´æˆ³
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # # ä¿å­˜åŸå§‹HTMLæ–‡ä»¶
            # html_file = f"ctrip_mobile_html_{timestamp}.html"
            # with open(html_file, 'w', encoding='utf-8') as f:
            #     f.write(result.html)
            # print(f"ğŸ’¾ HTMLæ–‡ä»¶å·²ä¿å­˜: {html_file}")
            
            # # ä¿å­˜Markdownæ–‡ä»¶
            # markdown_file = f"ctrip_mobile_markdown_{timestamp}.md"
            # with open(markdown_file, 'w', encoding='utf-8') as f:
            #     f.write(result.markdown)
            # print(f"ğŸ“ Markdownæ–‡ä»¶å·²ä¿å­˜: {markdown_file}")
            
            # æå–ç»“æ„åŒ–æ•°æ®
            print("ğŸ¯ å¼€å§‹æå–ç»“æ„åŒ–æ•°æ®...")
            product_data = extract_product_data_from_markdown(result.markdown)
            
            # # ä¿å­˜ç»“æ„åŒ–JSONæ•°æ®
            # json_file = f"ctrip_product_data_{timestamp}.json"
            # with open(json_file, 'w', encoding='utf-8') as f:
            #     json.dump(product_data, f, ensure_ascii=False, indent=2)
            # print(f"ğŸ“Š ç»“æ„åŒ–æ•°æ®å·²ä¿å­˜: {json_file}")
            
            # æ‰“å°æå–ç»“æœæ‘˜è¦
            print("\nğŸ¯ æ•°æ®æå–æ‘˜è¦:")
            print(f"å•†å“ID: {product_data['product_id']}")
            print(f"æ ‡é¢˜: {product_data['title']}")
            print(f"ä»·æ ¼: {product_data['price']}")
            print(f"å•†å“å›¾æ•°é‡: {len(product_data['product_images'])}")
            print(f"è¯¦æƒ…å›¾æ•°é‡: {len(product_data['detail_images'])}")
            print(f"ç‰¹è‰²æ•°é‡: {len(product_data['features'])}")
            
            return {
                "success": True,
                # "html_file": html_file,
                # "markdown_file": markdown_file,
                # "json_file": json_file,
                "product_data": product_data
            }
        else:
            print(f"âŒ é¡µé¢çˆ¬å–å¤±è´¥: {result.error_message}")
            return {"success": False, "error": result.error_message}

async def main():
    """
    ä¸»å‡½æ•° - æ‰§è¡Œå®Œæ•´çš„çˆ¬å–æµç¨‹
    """
    # æºç¨‹æ‰‹æœºç‰ˆå•†å“è¯¦æƒ…é¡µURL
    url = "https://m.ctrip.com/webapp/xtour/detail?rv=1&productid=61162192&departcityid=41&frompc=1&isRedirect=tour_h5"
    
    print("ğŸ¯ æºç¨‹æ‰‹æœºç‰ˆå•†å“è¯¦æƒ…çˆ¬è™« v3.0")
    print("=" * 50)
    
    result = await crawl_and_extract_ctrip_data(url)
    
    if result["success"]:
        print("\nâœ… çˆ¬å–ä»»åŠ¡å®Œæˆ!")
        # print(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶:")
        # print(f"  - HTML: {result['html_file']}")
        # print(f"  - Markdown: {result['markdown_file']}")
        # print(f"  - JSON: {result['json_file']}")
        print("\nğŸ‰ æ‰€æœ‰æ ¸å¿ƒæ•°æ®å·²æˆåŠŸæå–!")
    else:
        print(f"\nâŒ çˆ¬å–å¤±è´¥: {result['error']}")

if __name__ == "__main__":
    asyncio.run(main()) 