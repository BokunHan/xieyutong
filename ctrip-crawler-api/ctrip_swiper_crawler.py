import asyncio
import re
from datetime import datetime
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode


def extract_swiper_from_markdown(markdown_content, url=""):
    """
    ä»markdownå†…å®¹ä¸­æå–æ‰€æœ‰å›¾ç‰‡çš„URLåœ°å€
    """
    print("ğŸ¯ å¼€å§‹ä»markdownä¸­æå–å›¾ç‰‡URL...")

    # å°è¯•ä»URLä¸­è§£æproduct_id
    product_id_match = re.search(r'productId=(\d+)', url, re.IGNORECASE)
    product_id = product_id_match.group(1) if product_id_match else ""

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰æºç¨‹å›¾ç‰‡CDNçš„URL
    image_pattern = r'https?://dimg04\.c-ctrip\.com/images/[^)]+'
    image_urls = re.findall(image_pattern, markdown_content)

    # æ¸…ç†URLï¼Œå°è¯•è·å–é«˜æ¸…å¤§å›¾å¹¶å»é™¤é‡å¤é¡¹
    cleaned_urls = set()
    for img_url in image_urls:
        # ç§»é™¤å¸¸è§çš„ç¼©ç•¥å›¾/å°ºå¯¸å‚æ•°
        cleaned_url = re.sub(r'_R_\d+_\d+.*\.', '.', img_url)
        cleaned_url = re.sub(r'_C_\d+_\d+.*\.', '.', cleaned_url)
        cleaned_url = re.sub(r'_W_\d+_\d+.*\.', '.', cleaned_url)
        cleaned_urls.add(cleaned_url)

    print(f"âœ… æå–å®Œæˆï¼å…±æ‰¾åˆ° {len(cleaned_urls)} å¼ ä¸é‡å¤çš„å›¾ç‰‡ã€‚")

    # è¿”å›ç»“æ„åŒ–æ•°æ®
    return {
        "product_id": product_id,
        "image_urls": list(cleaned_urls),
        "metadata": {
            "extracted_at": datetime.now().isoformat(),
            "source_url": url
        }
    }


async def crawl_and_extract_swiper(url):
    """
    çˆ¬å–æºç¨‹å›¾ç‰‡åˆ—è¡¨é¡µé¢å¹¶æå–æ‰€æœ‰å›¾ç‰‡
    """
    print(f"ğŸš€ å¼€å§‹çˆ¬å–æºç¨‹å›¾ç‰‡åˆ—è¡¨é¡µé¢: {url}")

    browser_config = BrowserConfig(
        browser_type="chromium",
        headless=True,
        user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1"
    )

    # å›¾ç‰‡åˆ—è¡¨é¡µé¢å¾ˆå¯èƒ½ä¹Ÿéœ€è¦æ»šåŠ¨æ¥åŠ è½½å…¨éƒ¨å›¾ç‰‡
    crawl_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        page_timeout=60000,  # å»¶é•¿è¶…æ—¶æ—¶é—´åˆ°60ç§’
        delay_before_return_html=5,  # æ»šåŠ¨ç»“æŸåå†ç­‰å¾…5ç§’
        js_code="""
        async function fullScroll() {
            let maxScrolls = 40; // å…è®¸æ»šåŠ¨æ›´å¤šæ¬¡ä»¥åŠ è½½æ‰€æœ‰å›¾ç‰‡
            let scrollCount = 0;
            let scrollDelay = 800; // æ¯æ¬¡æ»šåŠ¨åç­‰å¾…800æ¯«ç§’

            while (scrollCount < maxScrolls) {
                let beforeHeight = document.body.scrollHeight;
                window.scrollTo(0, beforeHeight);
                await new Promise(resolve => setTimeout(resolve, scrollDelay));
                let afterHeight = document.body.scrollHeight;
                // å¦‚æœé¡µé¢é«˜åº¦ä¸å†å¢åŠ ï¼Œè¯´æ˜å·²åˆ°åº•éƒ¨
                if (afterHeight <= beforeHeight) {
                    break;
                }
                scrollCount++;
            }
        }
        await fullScroll();
        """
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        try:
            print("ğŸ“± æ­£åœ¨çˆ¬å–é¡µé¢å†…å®¹å¹¶æ¨¡æ‹Ÿæ»šåŠ¨...")
            result = await crawler.arun(url=url, config=crawl_config)

            if result.success and result.markdown:
                print("âœ… é¡µé¢çˆ¬å–æˆåŠŸï¼")
                structured_data = extract_swiper_from_markdown(result.markdown, url)
                return {"success": True, "data": structured_data}
            else:
                print(f"âŒ é¡µé¢çˆ¬å–å¤±è´¥: {result.error_message}")
                return {"success": False, "error": f"çˆ¬å–å¤±è´¥: {result.error_message}"}

        except Exception as e:
            print(f"ğŸ’¥ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            return {"success": False, "error": f"å‘ç”Ÿå¼‚å¸¸: {str(e)}"}