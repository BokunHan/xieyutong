import asyncio
import re
import json
from datetime import datetime
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode


def extract_reviews_from_markdown(markdown_content, url=""):
    """
    (æ­£å¼ç‰ˆ) ä»markdownå†…å®¹ä¸­æå–ç»“æ„åŒ–çš„è¯„è®ºæ•°æ®

    æ­¤å‡½æ•°æ ¹æ®å®é™…çš„ markdown å†…å®¹ç¼–å†™äº†è¯¦ç»†çš„è§£æè§„åˆ™ã€‚
    """
    print("ğŸ¯ (æ­£å¼ç‰ˆ) å¼€å§‹ä»markdownä¸­æå–è¯„è®ºæ•°æ®...")

    # å°è¯•ä»URLä¸­è§£æ product_id (å³ queryid)
    product_id_match = re.search(r'queryid=(\d+)', url, re.IGNORECASE)
    product_id = product_id_match.group(1) if product_id_match else ""

    # 1. åˆå§‹åŒ–æ•°æ®ç»“æ„
    data = {
        "product_id": product_id,
        "rating": 0.0,
        "good_rate": "0",
        "rating_spec": {
            "itinerary": "0",
            "accommodation": "0",
            "service": "0"
        },
        "reviews": [],
        "metadata": {
            "extracted_at": datetime.now().isoformat(),
            "source_url": url
        }
    }

    # 2. æå–é¡µé¢æ€»è¯„åˆ†
    # 5.0/5
    rating_match = re.search(r'(\d\.\d)/5', markdown_content)
    if rating_match:
        data["rating"] = float(rating_match.group(1))

    # å¥½è¯„ç‡100.0%
    good_rate_match = re.search(r'å¥½è¯„ç‡(\d+\.\d+)%', markdown_content)
    if good_rate_match:
        data["good_rate"] = good_rate_match.group(1)

    # æå–ä¸‰é¡¹è¯¦ç»†è¯„åˆ†
    # è¡Œç¨‹å®‰æ’
    # é…’åº—ä½“éªŒ
    # å¸å¯¼æœåŠ¡
    # 5.0
    # 5.0
    # 5.0
    spec_rating_match = re.search(
        r'è¡Œç¨‹å®‰æ’\s*\n\s*é…’åº—ä½“éªŒ\s*\n\s*å¸å¯¼æœåŠ¡\s*\n\s*(\d\.\d)\s*\n\s*(\d\.\d)\s*\n\s*(\d\.\d)',
        markdown_content
    )
    if spec_rating_match:
        data["rating_spec"]["itinerary"] = spec_rating_match.group(1)
        data["rating_spec"]["accommodation"] = spec_rating_match.group(2)
        data["rating_spec"]["service"] = spec_rating_match.group(3)
    else:
        print("âš ï¸ æœªæ‰¾åˆ°è¯¦ç»†è¯„åˆ†æ¨¡å—")

    # 3. é€è¡Œè§£æï¼Œæå–æ¯æ¡è¯„è®º
    lines = markdown_content.split('\n')

    # for line in lines:
    #     print(line)

    def remove_size_params(url):
        """å»æ‰å›¾ç‰‡URLä¸­çš„å°ºå¯¸å‚æ•°ï¼Œè·å–å¤§å›¾"""
        # å»æ‰ _C_æ•°å­—_æ•°å­—_R1_Q80 è¿™æ ·çš„å°ºå¯¸å‚æ•°
        url = re.sub(r'_[A-Za-z]_\d+_\d+(_R\d+)?(_Q\d+)?', '', url)
        return url

    current_review = None
    i = 0
    while i < len(lines):
        line = lines[i].strip()

        # æ£€æŸ¥æ˜¯å¦ä¸ºè¯„è®ºçš„èµ·å§‹ç‚¹ï¼ˆç”¨æˆ·å¤´åƒï¼‰
        if line.startswith('![](') and 'webp?proc=resize/m_w,w_40,h_40,2B60' in line:
            # å¦‚æœä¸Šä¸€æ¡è¯„è®ºè¿˜æœªä¿å­˜ï¼Œå…ˆä¿å­˜
            if current_review:
                data["reviews"].append(current_review)

            # åˆå§‹åŒ–ä¸€æ¡æ–°è¯„è®º
            current_review = {
                "user_avatar": line.replace('![](', '').replace(')', ''),
                "user_name": "N/A",
                "rating": 0.0,
                "created_at": "",
                "location": "",
                "group_type": "",
                "travel_date": "",
                "route": "",
                "content": "",
                "images": [],
                "guide_name": "",
                "guide_photo": "",
                "helpful_count": 0
            }

            # æå–ç”¨æˆ·å (å¤´åƒåä¸€è¡Œ)
            if i + 1 < len(lines):
                current_review["user_name"] = lines[i + 1].strip()

            # æå–è¯„åˆ† (å¤´åƒåä¸‰è¡Œ)
            if i + 3 < len(lines):
                rating_match = re.search(r'(\d\.\d)åˆ†', lines[i + 3])
                if rating_match:
                    current_review["rating"] = float(rating_match.group(1))

            # æå–å…ƒæ•°æ® (å¤´åƒåå››è¡Œ)
            # æ ¼å¼: 2024-02-14å‘è¡¨äºè¥¿è—å®¶åº­äº²å­2024-02-07å‡ºå‘æ—¥å–€åˆ™å£ç¢‘4é’»+ç å³°ä¾›æ°§æˆ¿
            if i + 4 < len(lines):
                meta_line = lines[i + 4].strip()
                meta_match = re.search(r'(\d{4}-\d{2}-\d{2})å‘è¡¨äº(\S+)(\S{4})(\d{4}-\d{2}-\d{2})å‡ºå‘(.+)', meta_line)
                if meta_match:
                    current_review["created_at"] = meta_match.group(1)
                    current_review["location"] = meta_match.group(2)
                    current_review["group_type"] = meta_match.group(3)
                    current_review["travel_date"] = meta_match.group(4)
                    current_review["route"] = meta_match.group(5)

            # æå–è¯„è®ºå†…å®¹ (å¤´åƒåäº”è¡Œ)
            if i + 5 < len(lines):
                current_review["content"] = lines[i + 5].strip()

            # è·³è¿‡è¯„è®ºå¤´ä¿¡æ¯ï¼Œç»§ç»­å¤„ç†
            i += 6
            continue

        # å¦‚æœå½“å‰æ­£åœ¨å¤„ç†ä¸€æ¡è¯„è®º
        if current_review:
            # æå–è¯„è®ºå›¾ç‰‡
            if line.startswith('![](') and 'C_360_360_Q70' in line:
                image_url = line.replace('![](', '').replace(')', '')
                current_review["images"].append(remove_size_params(image_url))

            # æå–å¸å¯¼ä¿¡æ¯
            elif line == 'TAçš„å¸å¯¼ï¼š':
                if i + 1 < len(lines):
                    guide_line = lines[i + 1].strip()

                    # æå–å¸å¯¼å¤´åƒå’Œåå­—
                    guide_match = re.search(r'!\[\]\((.*?\.jpg)\)([^!]+)', guide_line)
                    if guide_match:
                        current_review["guide_photo"] = guide_match.group(1)
                        current_review["guide_name"] = guide_match.group(2)

                i += 1  # è·³è¿‡å¸å¯¼è¡Œ

            # æ£€æŸ¥è¯„è®ºç»“æŸæ ‡å¿—ï¼ˆ"æœ‰ç”¨"å›¾æ ‡ï¼‰
            elif line.startswith('![](') and '0303y120009qrdtd8387B' in line:
                if i + 1 < len(lines):
                    helpful_line = lines[i + 1].strip()

                    if helpful_line.isdigit():
                        # å¦‚æœæ˜¯æ•°å­—ï¼Œç›´æ¥è½¬æ¢
                        current_review["helpful_count"] = int(helpful_line)
                    elif helpful_line == "æœ‰ç”¨":
                        # å¦‚æœæ˜¯ "æœ‰ç”¨" æ–‡å­—ï¼Œè®¡ä¸º 0
                        current_review["helpful_count"] = 0
                    else:
                        # å…œåº•ï¼Œä¸‡ä¸€æ ¼å¼æ˜¯ "æœ‰ç”¨ 10" è¿™ç§
                        count_match = re.search(r'(\d+)', helpful_line)
                        if count_match:
                            current_review["helpful_count"] = int(count_match.group(1))
                        else:
                            current_review["helpful_count"] = 0  # é»˜è®¤ 0

                data["reviews"].append(current_review)
                current_review = None  # é‡ç½®
                i += 1  # è·³è¿‡ "æœ‰ç”¨" æ–‡æœ¬è¡Œ
                continue  # ç»“æŸæœ¬æ¬¡å¾ªç¯

        i += 1

    # å¾ªç¯ç»“æŸåï¼Œä¿å­˜æœ€åä¸€æ¡è¯„è®ºï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if current_review:
        data["reviews"].append(current_review)

    print(f"âœ… (æ­£å¼ç‰ˆ) æå–å®Œæˆï¼å…±æ‰¾åˆ° {len(data['reviews'])} æ¡è¯„è®ºã€‚")

    # ç§»é™¤ä¸´æ—¶çš„åŸå§‹ markdown (å¦‚æœå­˜åœ¨)
    data.pop("raw_markdown_for_analysis", None)

    return data


async def crawl_and_extract_reviews(url):
    """
    çˆ¬å–æºç¨‹è¯„è®ºåˆ—è¡¨é¡µé¢ï¼Œè‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨ï¼Œç„¶åæå–æ•°æ®
    """
    print(f"ğŸš€ å¼€å§‹çˆ¬å–æºç¨‹è¯„è®ºé¡µé¢: {url}")

    # æ¨¡æ‹Ÿæ‰‹æœºæµè§ˆå™¨
    browser_config = BrowserConfig(
        browser_type="chromium",
        headless=True,
        viewport_width=375,
        viewport_height=812,
        user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1"
    )

    # çˆ¬å–é…ç½® - æ ¸å¿ƒæ˜¯ js_code
    crawl_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        page_timeout=240000,  # å»¶é•¿è¶…æ—¶åˆ°90ç§’ï¼Œä»¥åº”å¯¹å¤§é‡è¯„è®ºçš„æ»šåŠ¨
        delay_before_return_html=3.0,  # æ»šåŠ¨ç»“æŸåï¼Œå†ç­‰å¾…3ç§’ç¡®ä¿æ‰€æœ‰å†…å®¹æ¸²æŸ“
        log_console=True,  # æ‰“å°JSæ§åˆ¶å°æ—¥å¿—ï¼Œæ–¹ä¾¿è°ƒè¯•
        js_code="""
            async function scrollToBottomUntilEnd() {
                console.log('--- JS SCRIPT: å¼€å§‹æ»šåŠ¨åŠ è½½è¯„è®º... ---');
                let maxScrolls = 100;  // æœ€å¤§æ»šåŠ¨æ¬¡æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯
                let scrollCount = 0;
                let scrollDelay = 800; // æ¯æ¬¡æ»šåŠ¨åç­‰å¾…800æ¯«ç§’

                while (scrollCount < maxScrolls) {
                    // æ£€æŸ¥æ˜¯å¦å·²å‡ºç° "å·²ç»åˆ°åº•å•¦"
                    if (document.body.innerText.includes("å·²ç»åˆ°åº•å•¦")) {
                        console.log('--- JS SCRIPT: æ‰¾åˆ° "å·²ç»åˆ°åº•å•¦"ã€‚åœæ­¢æ»šåŠ¨ã€‚ ---');
                        break;
                    }
                    
                    // æ»šåŠ¨åˆ°åº•éƒ¨
                    window.scrollBy(0, 1200);
                    await new Promise(resolve => setTimeout(resolve, scrollDelay));
                    
                    scrollCount++;
                    console.log(`--- JS SCRIPT: æ»šåŠ¨æ¬¡æ•° ${scrollCount}/${maxScrolls} ---`);
                }

                if (scrollCount === maxScrolls) {
                    console.log('--- JS SCRIPT: è¾¾åˆ°æœ€å¤§æ»šåŠ¨æ¬¡æ•°ã€‚ ---');
                }
                
                console.log('--- JS SCRIPT: æ»šåŠ¨åŠ è½½å®Œæˆã€‚ ---');
            }
            
            // æ‰§è¡Œæ»šåŠ¨å‡½æ•°
            await scrollToBottomUntilEnd();
            """
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        try:
            print("ğŸ“± æ­£åœ¨åŠ è½½é¡µé¢å¹¶æ‰§è¡ŒJSæ»šåŠ¨è„šæœ¬...")
            result = await crawler.arun(url=url, config=crawl_config)

            if result.success and result.markdown:
                print("âœ… é¡µé¢çˆ¬å–æˆåŠŸ (å·²æ»šåŠ¨åˆ°åº•éƒ¨)ï¼")
                # (!!!) è°ƒç”¨æ­£å¼çš„è§£æå‡½æ•°
                structured_data = extract_reviews_from_markdown(result.markdown, url)
                return {"success": True, "data": structured_data}
            else:
                print(f"âŒ é¡µé¢çˆ¬å–å¤±è´¥: {result.error_message}")
                return {"success": False, "error": f"çˆ¬å–å¤±è´¥: {result.error_message}"}

        except Exception as e:
            print(f"ğŸ’¥ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            return {"success": False, "error": f"å‘ç”Ÿå¼‚å¸¸: {str(e)}"}


async def main():
    """
    ä¸»å‡½æ•° - æµ‹è¯•çˆ¬è™«åŠŸèƒ½
    """
    # ä½¿ç”¨ä½ æä¾›çš„ç¤ºä¾‹URL
    test_url = "https://m.ctrip.com/webapp/vacations/order/public/comment_list?channel=vacations-grp&queryid=45584321&scene=PRODUCT_QUERY"

    print("ğŸ¯ æºç¨‹è¯„è®ºçˆ¬è™«æµ‹è¯•å¼€å§‹")
    print(f"   URL: {test_url}")
    print("=" * 50)

    result = await crawl_and_extract_reviews(test_url)

    if result and result.get("success"):
        print("\nğŸ‰ çˆ¬å–æµ‹è¯•å®Œæˆï¼")

        # å°†è§£æåçš„ JSON ç»“æœä¿å­˜åˆ°æ–‡ä»¶
        output_filename = "ctrip_review_data.json"
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(result['data'], f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ è§£æåçš„ JSON æ•°æ®å·²ä¿å­˜åˆ°: {output_filename}")
        print("\nğŸ‘‰ ä½ ç°åœ¨å¯ä»¥é€šè¿‡ `main.py` çš„ `/api/reviews/{product_id}` æ¥å£è°ƒç”¨æ­¤åŠŸèƒ½äº†ã€‚")
    else:
        print(f"\nâŒ çˆ¬å–æµ‹è¯•å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")


if __name__ == "__main__":
    asyncio.run(main())