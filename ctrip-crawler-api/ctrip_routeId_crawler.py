import asyncio
import re
from datetime import datetime
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode


def extract_route_ids_from_html(html_content, url=""):
    """
    ä»HTMLå†…å®¹ä¸­æå–æ‰€æœ‰çš„ multiRouteId_xxxxxxxx çš„id
    """
    # print(html_content)
    print("ğŸ¯ å¼€å§‹ä»HTMLä¸­æå– route IDs...")

    # å°è¯•ä»URLä¸­è§£æproduct_id
    product_id_match = re.search(r'productid=(\d+)', url, re.IGNORECASE)
    product_id = product_id_match.group(1) if product_id_match else ""

    # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼
    route_id_pattern = re.compile(r'multiRouteId_(\d+)')

    # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„ID
    # Sä½¿ç”¨ set æ¥è‡ªåŠ¨å»é‡
    route_ids = set(route_id_pattern.findall(html_content))

    # è½¬æ¢ä¸ºåˆ—è¡¨
    route_ids_list = list(route_ids)

    print(f"âœ… æå–å®Œæˆï¼å…±æ‰¾åˆ° {len(route_ids_list)} ä¸ªä¸é‡å¤çš„ Route IDã€‚")

    a_route_pattern = re.compile(
        # 1. æŸ¥æ‰¾IDå¹¶æ•è·
        r'multiRouteId_(\d+)'
        # 2. éè´ªå©ªåœ°åŒ¹é…æ‰€æœ‰ä¸­é—´å†…å®¹
        r'.*?'
        # 3. åŒ¹é…é‚£ä¸ªåŒ…å« "Açº¿" æè¿°çš„spanæ ‡ç­¾
        r'<span class="[^"]*MultiRouteDescription_New[^"]*"[^>]*>'
        # 4. åŒ¹é…æ ‡ç­¾åçš„ä»»ä½•ç©ºæ ¼ï¼ˆåŒ…æ‹¬æ¢è¡Œç¬¦ï¼‰
        r'\s*'
        # 5. åŒ¹é…ä»¥ "Açº¿" å¼€å¤´çš„æ–‡æœ¬
        r'Açº¿',
        re.DOTALL | re.IGNORECASE  # re.DOTALL (è®©.åŒ¹é…æ¢è¡Œç¬¦)è‡³å…³é‡è¦
    )

    a_route_match = a_route_pattern.search(html_content)
    a_route_id = a_route_match.group(1) if a_route_match else route_ids_list[0]

    if a_route_id:
        print(f"âœ… æˆåŠŸåŒ¹é…åˆ° Açº¿ ID: {a_route_id}")
    else:
        print("âš ï¸ æœªèƒ½åŒ¹é…åˆ° Açº¿ IDã€‚")

    # è¿”å›ç»“æ„åŒ–æ•°æ®
    return {
        "product_id": product_id,
        "A_route_id": a_route_id,
        "route_ids": route_ids_list,
        "metadata": {
            "extracted_at": datetime.now().isoformat(),
            "source_url": url
        }
    }


async def crawl_and_extract_route_ids(url):
    """
    çˆ¬å–æºç¨‹å•†å“è¯¦æƒ…é¡µé¢å¹¶æå–æ‰€æœ‰ multiRouteId
    """
    print(f"ğŸš€ å¼€å§‹çˆ¬å–æºç¨‹è¯¦æƒ…é¡µé¢ (HTMLæ¨¡å¼): {url}")

    browser_config = BrowserConfig(
        headless=True,
        extra_args=['--disable-web-security', '--no-sandbox'],
    )

    # åªéœ€è¦åŸå§‹HTMLï¼Œä¸éœ€è¦å¤æ‚çš„JSæ»šåŠ¨ï¼Œæ‰€ä»¥é…ç½®ä¸€ä¸ªå¿«é€ŸåŠ è½½
    crawl_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        page_timeout=30000,
        delay_before_return_html=3.0,  # ç­‰å¾…3ç§’è®©åŸºç¡€JSåŠ è½½
        scan_full_page=False  # ä¸éœ€è¦å®Œæ•´æ‰«æ
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        try:
            print("ğŸ“± æ­£åœ¨åŠ è½½é¡µé¢...")
            result = await crawler.arun(url=url, config=crawl_config)

            # æ³¨æ„ï¼šæˆ‘ä»¬è¿™é‡Œä½¿ç”¨çš„æ˜¯ result.htmlï¼Œè€Œä¸æ˜¯ result.markdown
            if result.success and result.html:
                print("âœ… é¡µé¢HTMLçˆ¬å–æˆåŠŸï¼")
                structured_data = extract_route_ids_from_html(result.html, url)
                return {"success": True, "data": structured_data}
            else:
                print(f"âŒ é¡µé¢HTMLçˆ¬å–å¤±è´¥: {result.error_message}")
                return {"success": False, "error": f"çˆ¬å–å¤±è´¥: {result.error_message}"}

        except Exception as e:
            print(f"ğŸ’¥ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            return {"success": False, "error": f"å‘ç”Ÿå¼‚å¸¸: {str(e)}"}


# --- ç”¨äºç‹¬ç«‹æµ‹è¯• ---
async def main():
    """
    ä¸»å‡½æ•° - æµ‹è¯•çˆ¬è™«åŠŸèƒ½
    """
    # ä½¿ç”¨å’Œ detail_crawler ç›¸åŒçš„æµ‹è¯•ID
    test_url = "https://m.ctrip.com/webapp/xtour/detail?rv=1&productid=61162192&productId=61162192&isRedirect=tour_h5"

    print("ğŸ¯ æºç¨‹ Route ID çˆ¬è™«æµ‹è¯•å¼€å§‹")
    print("=" * 50)

    result = await crawl_and_extract_route_ids(test_url)

    if result and result.get("success"):
        print("\nğŸ‰ çˆ¬å–æµ‹è¯•å®Œæˆï¼")
        print("=" * 50)
        print("ğŸ“‹ æ•°æ®æ‘˜è¦:")
        print(f"   å•†å“ID: {result['data'].get('product_id', 'N/A')}")
        print(f"   Route IDs: {result['data'].get('route_ids', [])}")
    else:
        print(f"\nâŒ çˆ¬å–æµ‹è¯•å¤±è´¥: {result.get('error')}")


if __name__ == "__main__":
    asyncio.run(main())